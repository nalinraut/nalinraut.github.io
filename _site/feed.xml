<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-10-19T16:01:16-04:00</updated><id>http://localhost:4000/</id><title type="html">Nalin Raut</title><subtitle>Nalin Raut</subtitle><entry><title type="html">Traffic Sign Classifier</title><link href="http://localhost:4000/traffic-sign-classifier/" rel="alternate" type="text/html" title="Traffic Sign Classifier" /><published>2018-07-18T00:00:00-04:00</published><updated>2018-07-18T00:00:00-04:00</updated><id>http://localhost:4000/traffic-sign-classifier</id><content type="html" xml:base="http://localhost:4000/traffic-sign-classifier/">&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/intro.jpg&quot; alt=&quot;Intro&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;center&gt;&lt;h3&gt;Description&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; It is extremely necessary for a self-driving car to perceive all the traffic indications, mostly conveyed through traffic signs and act/ drive accordingly. In this project, I develop an algorithm to classify traffic signs.The project uses German Traffic Sign dataset. &lt;br /&gt;
Programming platform and Libraries: Python, OpenCV, Tensorflow, Pandas, Pickle.&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Methodology and Results &lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;For this project, I used the German Traffic Sign dataset for classification. Following table illustrates the dataset.
&lt;img src=&quot;/assets/img/trafficsignclassifier/1.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Following are randomly picked images with their labels each from a different class.
&lt;img src=&quot;/assets/img/trafficsignclassifier/output_8_0.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;training-set&quot;&gt;Training Set&lt;/h4&gt;

&lt;p&gt;The figure below illustrates number of image samples per class in the training set.
&lt;img src=&quot;/assets/img/trafficsignclassifier/output_9_0.png&quot; alt=&quot;Training Samples&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;validation-set&quot;&gt;Validation Set&lt;/h4&gt;

&lt;p&gt;The figure below illustrates number of image samples per class in the validation set.
&lt;img src=&quot;/assets/img/trafficsignclassifier/output_9_1.png&quot; alt=&quot;Validation Samples&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;testing-set&quot;&gt;Testing Set&lt;/h4&gt;

&lt;p&gt;The figure below illustrates number of image samples per class in the testing set.
&lt;img src=&quot;/assets/img/trafficsignclassifier/output_9_2.png&quot; alt=&quot;Testing Samples&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;image-pre-processing&quot;&gt;Image Pre-processing&lt;/h3&gt;

&lt;p&gt;The image is first normalized to have pixel values between -1.0 to 1.0 and also have zero mean. The images are trained relatively faster with the normalization.  Further, the image is converted to a grayscale color space from RGB color space. This reduces the breadth of the layering. The images are trained in batches and each batch consists of images shuffled to eliminate any bias.&lt;/p&gt;

&lt;h3 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h3&gt;

&lt;h4 id=&quot;design&quot;&gt;Design&lt;/h4&gt;

&lt;p&gt;LeNet is a popular classification architecture for digits, traffic signs, etc. My design consists of layers as tabulated below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/2.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;training-and-validation&quot;&gt;Training and Validation&lt;/h4&gt;

&lt;p&gt;As mentioned earlier the images are trained in batches. EPOCHS or episodes are run with asingle batch trained in it. Following are the parameters used for training.&lt;/p&gt;

&lt;p&gt;EPOCHS = 17 
After running 17 epochs there is no significant or no improvement in the accuracy.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; 
BATCH_SIZE    = 128
I trained the network model on a local CPU and hence preferred a low batch size of 128 images per batch.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
LEARNING RATE = 0.001 
Since Adam optimizer was used, a learning rate of 0.001 is suggested.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Following are the accuracies each for the training set, the last validation set and the testing set.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/3.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph below shows a trade off between the training and validation accuracies considering the number of episodes run.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/trade.png&quot; alt=&quot;Testing Samples&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;testing-on-unknown-images&quot;&gt;Testing on Unknown Images&lt;/h3&gt;

&lt;p&gt;I picked the following 5 unknown images for testing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/web.png&quot; alt=&quot;Testing Samples&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The prediction for the images are tabulated as follows :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/4.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The probabilities for individual labels for each image are as follows :&lt;/p&gt;

&lt;h4 id=&quot;1-image-1---keep-right&quot;&gt;1. Image 1 - Keep Right&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/5.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2-image-2---stop&quot;&gt;2. Image 2 - Stop&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/6.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3-image-3---speed-limit-30kmh&quot;&gt;3. Image 3 - Speed limit (30km/h)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/7.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4-image-4---pedestrians&quot;&gt;4. Image 4 - Pedestrians&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/8.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;5-image-5---no-entry&quot;&gt;5. Image 5 - No entry&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/trafficsignclassifier/9.png&quot; alt=&quot;Intro&quot; /&gt;&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; This project uses an architecture similar to LeNet for training purpose. An accuracy of 92.90% is obtained for the test set. The accuracy can be improved using batch normalization after every convolutional layer in the neural network model.  &lt;/p&gt;

&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;ul id=&quot;horizontal-list&quot;&gt;
&lt;li class=&quot;display: inline&quot;&gt;
&lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/nalinraut/CarND-Traffic_Sign_Classifier&quot;&gt;Code Repository Link &lt;i class=&quot;fa fa-fw fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt;
                                
                                
&lt;!-- &lt;li&gt;
&lt;strong&gt;&lt;a href=&quot;javascript:void(0);&quot;&gt;Project-Report&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt; --&gt;
                                
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Computer Vision - Deep Learning</summary></entry><entry><title type="html">Finding Lane Lines on Road</title><link href="http://localhost:4000/finding-lane-lines/" rel="alternate" type="text/html" title="Finding Lane Lines on Road" /><published>2018-06-12T00:00:00-04:00</published><updated>2018-06-12T00:00:00-04:00</updated><id>http://localhost:4000/finding-lane-lines</id><content type="html" xml:base="http://localhost:4000/finding-lane-lines/">&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/lanelines/video.gif&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;center&gt;&lt;h3&gt;Description&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; When we drive, we use our eyes to decide where to go. The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle. In this project, I develop an algorithm to detect lane lines automatically.&lt;br /&gt;
Programming platform and Libraries: Python and OpenCV.&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Methodology&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;!-- &lt;p style=&quot;text-align: justify&quot;&gt;Firstly, a valid maze is generated using Depth-First Search Algorithm.
                            Further, robot follows the undermentioned policy,&lt;br/&gt;
                            • Each robot starts exploring those cells which have not been previously explored by itself  and any other robots.&lt;br/&gt;
                            • While exploring, if a robot detects a junction which is a cell with two or more branches, the robot arbitrarily chooses a direction for further exploration and stores the junction as potentially unexplored node.&lt;br/&gt;
                            • While exploring the maze, if the robot encounters a dead-end or enters a cell already explored by another robot, the robot back-tracks to the nearest unexplored cell.&lt;br/&gt;
                            • All robots continue their exploration until all potentially unexplored cells in their respective lists are visited.&lt;br/&gt;
                            • Since all robots are continuously communicating with the common server, they get the completely mapped maze in the end which will further be used to travel to the goal node.&lt;br/&gt;&lt;/p&gt; --&gt;
&lt;h3 id=&quot;the-pipeline&quot;&gt;The Pipeline&lt;/h3&gt;
&lt;p&gt;My pipelines consists of the following steps:&lt;/p&gt;

&lt;h4 id=&quot;i-converting-the-input-image-to-hls-from-rgb&quot;&gt;(i) Converting the input image to HLS from RGB:&lt;/h4&gt;
&lt;p&gt;The &lt;i&gt;toHLS()&lt;/i&gt; function converts the image to HLS scale and masks the image with yellow and white filters. The HLS       domain takes care of the variation in the lighting conditions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lanelines/hsv_img.png&quot; alt=&quot;HLS Image&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;ii-converting-the-hsl-masked-image-to-grayscale&quot;&gt;(ii) Converting the HSL masked image to grayscale:&lt;/h4&gt;
&lt;p&gt;The image is further converted into an grayscale image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lanelines/gray_img.png&quot; alt=&quot;Grayscale&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;iii-applying-guassian-blur&quot;&gt;(iii) Applying Guassian Blur:&lt;/h4&gt;
&lt;p&gt;Further, a Guassian Blur filter with kernel size 15 is used to remove noise.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lanelines/gray_blur.png&quot; alt=&quot;Gaussian Blur&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;iv-perform-canny-edge-detection&quot;&gt;(iv) Perform Canny Edge Detection:&lt;/h4&gt;
&lt;p&gt;Next the grayscaled image is fed into a canny edge detector and edges are obtained. A double threshold is used (low threshold = 100 and high threshold = 200) with low threshold to high threshold ratio between 1/2 p 1/3 as suggested.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lanelines/edges.png&quot; alt=&quot;Canny&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;v-extract-region-of-interest&quot;&gt;(v) Extract Region of Interest:&lt;/h4&gt;
&lt;p&gt;Edges are detected all over the image, however, the edges that matter are in a particular region therefore the polygonal region is extracted and only those edges that fall in that region are kept while rest are discarded.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lanelines/roi1.png&quot; alt=&quot;Canny with RoI&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;vi-draw-hough-lines-on-the-edge-image&quot;&gt;(vi) Draw Hough Lines on the Edge Image:&lt;/h4&gt;
&lt;p&gt;The image with edges is used to draw Hough lines. The function basically converts the points in the image space to lines inthe Hough space. For drawing lines I use the &lt;i&gt;modified_draw_lines()&lt;/i&gt; function to ensure several line segments are extrapolated/ averaged to one line segment per lane line, also ensuring the smoothness.
The raw line segments (more than one per lane line) are broken and need to be averaged as well as extrapolated and for this, I find the slope of each line segment and further classify the points on the line segments to be part of either positive slope or negative slope. Here the line segments with positive slope belong to the right lane line and line segments with negative slope belong to the left lane line. To fit a line, &lt;i&gt;polyfit()&lt;/i&gt; with 1 degree is used and further the lane lines are drawn. A global list with a fixed length is maintained to which the coordinates are appended. This helps the algorithm to also keep track of previous points. This way, the broken lines can be extrapolated/ averaged to a single line.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lanelines/extrapolated.png&quot; alt=&quot;Extrapolate&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;vii-extract-region-of-interest&quot;&gt;(vii) Extract Region of Interest:&lt;/h4&gt;
&lt;p&gt;Again, the extrapolated lane lines are drawn on the entire image. To avoid this a region of interest masks the image for the lane lines appear only in the region that matters.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lanelines/final_extrapolate.png&quot; alt=&quot;Extrapolate with RoI&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;viii-weighted-image&quot;&gt;(viii) Weighted Image:&lt;/h4&gt;
&lt;p&gt;The image with extrpolated lines goes on the original image and forms the final image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lanelines/image.jpg&quot; alt=&quot;Final Image&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;center&gt;&lt;h3&gt;Results&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot;&gt;
&lt;p style=&quot;text-align: justify&quot;&gt; &lt;/p&gt; --&gt;

&lt;center&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; This project is a very primitive way of detecting lane lines. The shortcoming of this project is the need to manually tune a number of parameters that can be done using auto-calibration. Also, the algorithm can be improved using a quadratic curve fitting techinique encompassing the edge points. This will provide with better results. &lt;/p&gt;

&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;ul id=&quot;horizontal-list&quot;&gt;
&lt;li class=&quot;display: inline&quot;&gt;
&lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/nalinraut/CarND-Lane_Lines&quot;&gt;Code Repository Link &lt;i class=&quot;fa fa-fw fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt;
                                
                                
&lt;!-- &lt;li&gt;
&lt;strong&gt;&lt;a href=&quot;javascript:void(0);&quot;&gt;Project-Report&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt; --&gt;
                                
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Computer Vision</summary></entry><entry><title type="html">3 Dimensional Rapidly Exploring Random Trees Algorithm on Fetch Robot</title><link href="http://localhost:4000/3D-RRT-Robot-Manipulator/" rel="alternate" type="text/html" title="3 Dimensional Rapidly Exploring Random Trees Algorithm on Fetch Robot" /><published>2018-04-30T00:00:00-04:00</published><updated>2018-04-30T00:00:00-04:00</updated><id>http://localhost:4000/3D-RRT-Robot-Manipulator</id><content type="html" xml:base="http://localhost:4000/3D-RRT-Robot-Manipulator/">&lt;p&gt;&lt;img src=&quot;/assets/img/mp1.gif&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt; &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Description&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; This project uses Open AI's Robotics Environment for a Motion Planning problem. Rapidly exploring Random Trees (RRT) algorithm is implemented in a 3- Dimensional Space. Motion planing algoritm is implemented in the Fetch Robot Manipulator and the end-effector follows the planned path from a start to goal location. &lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Methodology&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;
• Implemented 3-Dimensional RRT in a 3D space.&lt;br /&gt;
• Initiated the space with start and goal location.&lt;br /&gt;
• Fed the obtained path to the OpenAI’s Gym and made the end    effector follow both the extend and connect version of RRT.&lt;br /&gt;
• Observed the path followed by the end effector in the space with and without obstacle.&lt;br /&gt;
• Computed the total cost of both extend and connect path for spaces with and without obstacles.&lt;br /&gt;
&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; We compare the cost of both paths, the generated one and the followed one. Ofcourse, the robot won't take the generated path because it is generated and more complex. It would rather follow a simple path with minimum breaks.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;The figure below shows both, 1. The path randomly generetaed in red, and 2. The path followed in green for 3 dimensional space without any obstacles.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/3drrt1.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; The table below shows cost incurred for paths in a 3D space without obstacle.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/3drrt1r.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;The figure below shows both, 1. The path randomly generetaed in red, and 2. The path followed in green for 3 dimensional space with obstacle.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/3drrt2.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; The table below shows cost incurred for paths in a 3D space with obstacle.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/3drrt2r.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;ul class=&quot;list-inline item-details&quot;&gt;
                                
&lt;li&gt;
&lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/nalinraut/Planning-Algorithms/tree/master/3D_RRT&quot;&gt;Code Repository Link &lt;i class=&quot;fa fa-fw fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;&lt;a href=&quot;https://github.com/nalinraut/Planning-Algorithms/blob/master/3D_RRT/Report.pdf&quot;&gt;Project-Report&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Motion Planning</summary></entry><entry><title type="html">Virtual Haptic Environment</title><link href="http://localhost:4000/virtual-haptic-environment/" rel="alternate" type="text/html" title="Virtual Haptic Environment" /><published>2018-04-23T00:00:00-04:00</published><updated>2018-04-23T00:00:00-04:00</updated><id>http://localhost:4000/virtual-haptic-environment</id><content type="html" xml:base="http://localhost:4000/virtual-haptic-environment/">&lt;p&gt;&lt;img src=&quot;/assets/img/dyn.gif&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt; &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;center&gt;&lt;h3&gt;Description&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;With the advancement of technology, robot assisted surgical systems are developed to overcome the limitations of pre-existing minimally-invasive surgical procedures. However, these surgical systems lack real time force feedback during the operation. Haptics and force feedback allow a surgeon to make more controlled movements in simulation and reality, but so far neither has been extensively implemented. This paper aimed to model a virtual gripper to interact with dynamic bodies present in the environment and calculate simulated tip forces to transfer back to the haptic device and subsequently the user. The force feedback produced at the gripper is translated to the haptic device, enabling the user to feel the forces generated at the gripper. The simulated environment, dynamical calculations, and feedback were tested using the haptic device controlling the modelled gripper by holding, moving, and acting upon bodies present in the virtual environment.&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Methodology&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;
The idea of virtual haptic environment for surgical simula-
tion has been realized through the integration of several open
source softwares, the most crucial of them being CHAI3D,
ROS, OpenGL, and Bullet Physics engine. Chai3D is the most
effective haptic research software which has been integrated
with ROS to establish communication between the virtual
haptic environment and the physical haptic devices like da
vinci , Geomagic Touch and Novint Falcon. The CHAI3D
library internally uses OpenGL and Bullet physics engine
to model the computer graphics and simulate the physics
respectively. A high level OpenGL wrapper library called
GLFW has been used to develop the modules and virtual
environments as it allows easy usage of the rather complex
OpenGL library. The Bullet physics engine was used to model
the physical interactions between the objects for example in
the pick and place module discussed in the section IV-C.
The CHAI3D library automatically identifies the haptic
devices connected to the computer and spawns the tool in the
virtual environment which is set while developing the module.
Specifically for the MTM(master tool manipulator) of DVRK,
the tool that represents the haptic device in the environment is
a gripper as shown in Fig. 6. This gripper mimics the MTM
and can be opened and closed by pinching action performed
on the MTM. The haptic tool used for the Geomagic touch
device for the purpose of incision practice is a knife or a thin
surgical tool as mentioned in section IV-B as the manipulator
of Geomagic touch is a stylus having 6 Degrees of Freedom.
For establishing the communication between Novint Fal-
con and CHAI-ROS bridge, ros-falcon Catkin package was
installed along with the device drivers. Although CHAI3D
detects to Novint Falcon automatically, this ROS package
allows the Falcon to interact with the virtual DVRK as the
DVRK library has been developed as a ROS package. This
package allowed the easy manipulability of gripper using the
Falcon device while receiving accurate haptic feedback. The
pick and place module discussed in section IV-C relies upon
the falcon device with the modified gripper design.
Using the Geomagic touch device with CHAI3D is a
challenge as the device is old and not supported directly
by the CHAI3D library. Although there exists procedures to
install the device kernels for the CHAI environment to detect
it, we were unsuccessful in doing so because of operating
system compatibility issues. Also, the native kernel would
not provide ROS interfacing. To tackle this situation, the
Geomagic ROS package was installed and a teleoperation
module was developed from scratch to interface the Geomagic
touch with the virtual MTML device in the haptic simulation.
The Geomagic ROS package provides the ROS topics for pose,
force and twist that were used to establish its communication
with the MTML as the DVRK ROS package provides similar
ROS topics. This module aided us in realizing the incision
practice task as discussed in section IV-B.
Fig 5 shows the high level architecture of all the developed
modules tied together establishing communication between the
devices. The gt talker ROS node interfaces the Geomagic
touch and virtual MTML kinematics whereas gt force is
responsible for transmitting the force feedback from the virtual
environment to the Geomagic Touch physical device. Although
the kinematics teleoperation worked as expected after scaling
down the device workspace to fit in the virtual environment,
the force feedback received was inaccurate and inconsistent
because of the control law that need to be rewritten in order
to allow accurate force feedback to the user while using
Geomagic device in the incision practice module that was
created in this project.
The ROS topics at the left-most side shoes the nodes for
keyboard teleoperation. The chai env node is the main ROS
node that interfaces CHAI3D with ROS-DVRK that allows the
haptic devices to interact with the virtual gripper and knife
tools in the CHAI3D environment.
The ROS wrapped architecture is highly flexible in terms of
programming languages and provides lot of useful function-
alities otherwise unavailable. It allows the inclusion of new
haptic devices and peripherals to the system easily as ROS
is language independent middleware and possess the utilities
required for seamless integration.&lt;/p&gt;

&lt;!-- &lt;center&gt;&lt;h3&gt;Results&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot;&gt;
&lt;p style=&quot;text-align: justify&quot;&gt; The results displayed in figures below, depict the average number of steps taken by the robots in order to explore and map the complete maze. It can be observed that the average number of steps reduces exponentially with increase in number of robots. It can also be observed that with increase in number of robots and maze size, the factor of load imbalance comes into picture. While the load imbalance doesn’t represent a particuar trend like Average number of steps, we still can logically deduce that the probability of load imbalance increases with increasing robots. &lt;/p&gt;

![image-title-here](/assets/img/ai_result1.png){:class=&quot;img-responsive width=176 height=71&quot;}  &lt;br/&gt;&lt;br/&gt;

![image-title-here](/assets/img/ai_results2.png){:class=&quot;img-responsive width=176 height=71&quot;}  &lt;br/&gt;&lt;br/&gt;

 



&lt;center&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot;&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;We present a problem where a swarm of robots is required to map a maze. We introduce a unique multi-robot approach, which enhances Recursive Backtracking algorithm to the multi-robot case. The average number of steps taken in case of multi robot system as compared to single robot decrease effectively. Another factor of load imbalance i.e. a considerable difference in number of steps taken by individual robots is also observed with increase in size of maze and number of robots. The approach mentioned above, also allowsto map the maze from any starting locations for the robots, thus removing the restriction of starting all robots from a single location in the maze. We hope to scale this maze mapping and shortest path planning algorithm from 2D mazes to 3D mazes that will depict different floors of a multi storey building.&lt;/p&gt; --&gt;

&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;ul id=&quot;horizontal-list&quot;&gt;
&lt;li class=&quot;display: inline&quot;&gt;
&lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/nalinraut/daVinci_haptics&quot;&gt;Code Repository Link &lt;i class=&quot;fa fa-fw fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt;
                                
                                
&lt;!-- &lt;li&gt;
&lt;strong&gt;&lt;a href=&quot;javascript:void(0);&quot;&gt;Project-Report&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt; --&gt;
                                
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Robot Dynamics</summary></entry><entry><title type="html">Scene Recognition using Transfer Learning</title><link href="http://localhost:4000/scene-recognition/" rel="alternate" type="text/html" title="Scene Recognition using Transfer Learning" /><published>2017-12-10T00:00:00-05:00</published><updated>2017-12-10T00:00:00-05:00</updated><id>http://localhost:4000/scene-recognition</id><content type="html" xml:base="http://localhost:4000/scene-recognition/">&lt;p&gt;&lt;img src=&quot;/assets/img/cv1.jpeg&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt; &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;center&gt;&lt;h3&gt;Description&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;The project includes scene recognition including different scene categories such as: Kitchen, Bedroom, and Corridor. Furthermore, implementing different classification techniques and comparing them is also part of our study. The project attempts to solve this problem using Convolutional Neural Networks. This project also utilizes and compares various approaches such as Transfer Learning and the bag-of-words model for the purpose of indoor scene recognition.&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Methodology&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;- Implemented Transfer Learning using a pre-trained Convolutional Neural Network. &lt;br /&gt; 
- Additionally, designed a Convolutional Neural Network model from scratch in MATLAB &lt;br /&gt;
- Compared the above mentioned approaches, earlier implemented Nearest Neighbour and SVM classifier approaches.&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Results&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; The results of the study are listed in the table below. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/tabular_CV.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;I. Confusion Matrix (Bag of Features with Nearest Neighbor Classifier). The figure below shows confusion matrix for scene classification using Bag of Features with Nearest Neighbor Classifier: &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/BoW_NN.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;II. Confusion Matrix (Bag of Features with SVM Classifier). The figure below shows confusion matrix for scene classification using Bag of Features with SVM Classifier: &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/BoW_SVM.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;III. Confusion Matrix (CNN built from scratch). The figure below shows confusion matrix for scene classification using CNN designed from scratch : &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/CNN_scratch.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;IV. Confusion Matrix (Transfer Learning with a Pre-Trained CNN). The figure below shows confusion matrix for scene classification using Transfer Learning with a pre-trained network.: &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/CNN_TL.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt;&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;Through this project, we learned all about the intricacies of convolutional neural networks and their use in image recognition tasks. Based on the comparative results, we concluded that Neural Networks perform better than other classification methods used in this project for the purpose of indoor scene recognition. We also saw that the bag-of-words model was more confident in classifying images from the kitchen, whereas the CNNs were most confident about classifying corridors. Another important observation was that to improve the accuracy of CNNs, a huge dataset of objects or scenes is required.&lt;/p&gt;

&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;ul id=&quot;horizontal-list&quot;&gt;
&lt;li class=&quot;display: inline&quot;&gt;
&lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/nalinraut/Indoor-Scene-Recognition&quot;&gt;Code Repository Link &lt;i class=&quot;fa fa-fw fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt;
                                
                                
&lt;li&gt;
&lt;strong&gt;&lt;a href=&quot;javascript:void(0);&quot;&gt;Project-Report&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt;
                                
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Computer Vision</summary></entry><entry><title type="html">Robot Teleoperation by Inertial Measurement Unit</title><link href="http://localhost:4000/teleoperation-using-imu/" rel="alternate" type="text/html" title="Robot Teleoperation by Inertial Measurement Unit" /><published>2017-12-01T00:00:00-05:00</published><updated>2017-12-01T00:00:00-05:00</updated><id>http://localhost:4000/teleoperation-using-imu</id><content type="html" xml:base="http://localhost:4000/teleoperation-using-imu/">&lt;p&gt;&lt;img src=&quot;/assets/img/imu.gif&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt;&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Description&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;Human safety has always been an issue in hazardous environmental working in any field. To overcome these problems of human safety robot technology can be proved to be a boon. Teleoperation can be a very good way to operate robots from a distance and at the same time keeping the human operator in a safe distant environment. Hence, the most basic way to do this is using an IMU Inertial Measurement Unit, which gives us 9 degrees of freedom. So the flexibility of this particular sensor enables us to operate a robot easily. Our project involves a robot in simulation, which is a 4 degree  freedom arm that is operated by the IMU sensor using an arduino board as our controller. We use the gazebo simulator for our simulation and also the Robot Operating System (ROS) for the communication between the IMU sensor and the robot. The robot uses the IMU signals to move its arm and push an object off the table.&lt;/p&gt;

&lt;h3&gt;Methodology&lt;/h3&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;
* At first, the arduino collects the sensor data from IMU sensor.&lt;br /&gt;
* Now, the arduino sketch(c++ code) running on arduino board publish the sensor data in form of a geometry_msg (imu_msg) to the ROS Topic (/team7/imu_data)&lt;br /&gt;
* The python program (pub_to_arm.py) subscribes to the topic “/team7/imu_data”, processes the data and create a message to publish to the ROS Topic/rrbot/joint1_position_controller/command.&lt;br /&gt;
* Over the ROS controller package, this message simulates the robot arm movement in Gazebo.&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/RBE500_methodology.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt;&lt;/p&gt;

&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;ul class=&quot;list-inline item-details&quot;&gt;
                                
    &lt;li&gt;
        &lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/nalinraut/Indoor-Scene-Recognition&quot;&gt;Code Repository Link &lt;i class=&quot;fa fa-fw fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;
        &lt;/strong&gt;
    &lt;/li&gt;
                                
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Foundation of Robotics</summary></entry><entry><title type="html">Multi-Robot Maze Mapping</title><link href="http://localhost:4000/multi-robot-maze-mapping/" rel="alternate" type="text/html" title="Multi-Robot Maze Mapping" /><published>2017-04-06T00:00:00-04:00</published><updated>2017-04-06T00:00:00-04:00</updated><id>http://localhost:4000/multi-robot-maze-mapping</id><content type="html" xml:base="http://localhost:4000/multi-robot-maze-mapping/">&lt;p&gt;&lt;img src=&quot;/assets/img/AI_project.gif&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt; &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;center&gt;&lt;h3&gt;Description&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;Swarm robotics nowadays, has become one of the most popular topics among researchers. Maze exploration using swarm robots is a field which has potentially multiple applications in daily life. This project introduces a novel approach to map the maze using a swarm of robots, and further, use this mapped maze to determine the closest path to the goal node depending on the application in which the system is being used. Thus, using an optimized Recursive Backtracking algorithm and a Client-Server Synchronization architecture, we show that using multiple robots to map a large area or maze reduces overall time as well the average number of steps taken by each robot. We also go on to implement A* algorithm on the fully mapped maze to find which robot is nearest to the goal location and&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Methodology&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;Firstly, a valid maze is generated using Depth-First Search Algorithm.
                            Further, robot follows the undermentioned policy,&lt;br /&gt;
                            • Each robot starts exploring those cells which have not been previously explored by itself  and any other robots.&lt;br /&gt;
                            • While exploring, if a robot detects a junction which is a cell with two or more branches, the robot arbitrarily chooses a direction for further exploration and stores the junction as potentially unexplored node.&lt;br /&gt;
                            • While exploring the maze, if the robot encounters a dead-end or enters a cell already explored by another robot, the robot back-tracks to the nearest unexplored cell.&lt;br /&gt;
                            • All robots continue their exploration until all potentially unexplored cells in their respective lists are visited.&lt;br /&gt;
                            • Since all robots are continuously communicating with the common server, they get the completely mapped maze in the end which will further be used to travel to the goal node.&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Results&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt; The results displayed in figures below, depict the average number of steps taken by the robots in order to explore and map the complete maze. It can be observed that the average number of steps reduces exponentially with increase in number of robots. It can also be observed that with increase in number of robots and maze size, the factor of load imbalance comes into picture. While the load imbalance doesn’t represent a particuar trend like Average number of steps, we still can logically deduce that the probability of load imbalance increases with increasing robots. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ai_result1.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ai_results2.png&quot; alt=&quot;image-title-here&quot; class=&quot;img-responsive width=176 height=71&quot; /&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;/center&gt;
&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;p style=&quot;text-align: justify&quot;&gt;We present a problem where a swarm of robots is required to map a maze. We introduce a unique multi-robot approach, which enhances Recursive Backtracking algorithm to the multi-robot case. The average number of steps taken in case of multi robot system as compared to single robot decrease effectively. Another factor of load imbalance i.e. a considerable difference in number of steps taken by individual robots is also observed with increase in size of maze and number of robots. The approach mentioned above, also allowsto map the maze from any starting locations for the robots, thus removing the restriction of starting all robots from a single location in the maze. We hope to scale this maze mapping and shortest path planning algorithm from 2D mazes to 3D mazes that will depict different floors of a multi storey building.&lt;/p&gt;

&lt;hr class=&quot;star-primary&quot; /&gt;

&lt;ul id=&quot;horizontal-list&quot;&gt;
&lt;li class=&quot;display: inline&quot;&gt;
&lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://github.com/nalinraut/Multi-Robot-Maze-Mapping&quot;&gt;Code Repository Link &lt;i class=&quot;fa fa-fw fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt;
                                
                                
&lt;!-- &lt;li&gt;
&lt;strong&gt;&lt;a href=&quot;javascript:void(0);&quot;&gt;Project-Report&lt;/a&gt;
&lt;/strong&gt;
&lt;/li&gt; --&gt;
                                
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Artificial Intelligence</summary></entry><entry><title type="html">Moon Jekyll Theme</title><link href="http://localhost:4000/moon-theme/" rel="alternate" type="text/html" title="Moon Jekyll Theme" /><published>2016-04-06T00:00:00-04:00</published><updated>2016-04-06T00:00:00-04:00</updated><id>http://localhost:4000/moon-theme</id><content type="html" xml:base="http://localhost:4000/moon-theme/">&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/754514/14509720/61c61058-01d6-11e6-93ab-0918515ecd56.png&quot; alt=&quot;Moon Homepage&quot; /&gt;&lt;/p&gt;

&lt;center&gt;&lt;b&gt;Moon&lt;/b&gt; is a minimal, one column jekyll theme.&lt;/center&gt;

&lt;p&gt;I’m not a developer or designer. And I don’t add footer to show who did this theme. If you like this theme or using it, please give a &lt;strong&gt;star&lt;/strong&gt; for motivation, It makes me happy.&lt;/p&gt;

&lt;iframe src=&quot;https://ghbtns.com/github-btn.html?user=TaylanTatli&amp;amp;repo=Moon&amp;amp;type=star&amp;amp;count=true&amp;amp;size=large&quot; frameborder=&quot;0&quot; scrolling=&quot;0&quot; width=&quot;160px&quot; height=&quot;30px&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Fork the &lt;a href=&quot;https://github.com/TaylanTatli/Moon/fork&quot;&gt;Moon repo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Edit &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; file.&lt;/li&gt;
  &lt;li&gt;Remove sample posts from &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; folder and add yours.&lt;/li&gt;
  &lt;li&gt;Edit &lt;code class=&quot;highlighter-rouge&quot;&gt;index.md&lt;/code&gt; file in &lt;code class=&quot;highlighter-rouge&quot;&gt;about&lt;/code&gt; folder.&lt;/li&gt;
  &lt;li&gt;Change repo name to &lt;code class=&quot;highlighter-rouge&quot;&gt;YourUserName.github.io&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That’s all.&lt;/p&gt;

&lt;h2 id=&quot;preview&quot;&gt;Preview&lt;/h2&gt;

&lt;figure class=&quot;third&quot;&gt;
    
    &lt;a href=&quot;https://cloud.githubusercontent.com/assets/754514/14509716/61ac6c8e-01d6-11e6-879f-8308883de790.png&quot;&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/754514/14509716/61ac6c8e-01d6-11e6-879f-8308883de790.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
    
    &lt;a href=&quot;https://cloud.githubusercontent.com/assets/754514/14509717/61ad05ae-01d6-11e6-85ae-5a817dd8763b.png&quot;&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/754514/14509717/61ad05ae-01d6-11e6-85ae-5a817dd8763b.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
    
    &lt;a href=&quot;https://cloud.githubusercontent.com/assets/754514/14509714/61a89708-01d6-11e6-8fcd-74b002a060df.png&quot;&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/754514/14509714/61a89708-01d6-11e6-8fcd-74b002a060df.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
    
    &lt;figcaption&gt;Screenshots of Moon Theme&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;hr /&gt;

&lt;figure class=&quot;half&quot;&gt;
    
    &lt;a href=&quot;https://cloud.githubusercontent.com/assets/754514/14509718/61b09a20-01d6-11e6-8da1-4202ae4d83cd.png&quot;&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/754514/14509718/61b09a20-01d6-11e6-8da1-4202ae4d83cd.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
    
    &lt;a href=&quot;https://cloud.githubusercontent.com/assets/754514/14509715/61aa9d00-01d6-11e6-81a6-c6837edf2e84.png&quot;&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/754514/14509715/61aa9d00-01d6-11e6-81a6-c6837edf2e84.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
    
    &lt;figcaption&gt;Moon Theme on Small Screen Size&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;See a &lt;a href=&quot;http://taylantatli.github.io/Moon&quot;&gt;live version of Moon&lt;/a&gt; hosted on GitHub.&lt;/p&gt;

&lt;h2 id=&quot;site-setup&quot;&gt;Site Setup&lt;/h2&gt;
&lt;p&gt;A quick checklist of the files you’ll want to edit to get up and running.&lt;/p&gt;

&lt;h3 id=&quot;site-wide-configuration&quot;&gt;Site Wide Configuration&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; is your friend. Open it up and personalize it. Most variables are self explanatory but here’s an explanation of each if needed:&lt;/p&gt;

&lt;h4 id=&quot;title&quot;&gt;title&lt;/h4&gt;

&lt;p&gt;The title of your site… shocker!&lt;/p&gt;

&lt;p&gt;Example &lt;code class=&quot;highlighter-rouge&quot;&gt;title: My Awesome Site&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;bio&quot;&gt;bio&lt;/h4&gt;

&lt;p&gt;The description to show on your homepage.&lt;/p&gt;

&lt;h4 id=&quot;description&quot;&gt;description&lt;/h4&gt;

&lt;p&gt;The description to use for meta tags and navigation menu.&lt;/p&gt;

&lt;h4 id=&quot;url&quot;&gt;url&lt;/h4&gt;

&lt;p&gt;Used to generate absolute urls in &lt;code class=&quot;highlighter-rouge&quot;&gt;sitemap.xml&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;feed.xml&lt;/code&gt;, and for generating canonical URLs in &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;head&amp;gt;&lt;/code&gt;. When developing locally either comment this out or use something like &lt;code class=&quot;highlighter-rouge&quot;&gt;http://localhost:4000&lt;/code&gt; so all assets load properly. &lt;em&gt;Don’t include a trailing &lt;code class=&quot;highlighter-rouge&quot;&gt;/&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Examples:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://taylantatli.me/Moon&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://localhost:4000&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;//cooldude.github.io&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;reading_time&quot;&gt;reading_time&lt;/h4&gt;

&lt;p&gt;Set true to show reading time for posts. And set &lt;code class=&quot;highlighter-rouge&quot;&gt;words_per_minute&lt;/code&gt;, default is 200.&lt;/p&gt;

&lt;h4 id=&quot;logo&quot;&gt;logo&lt;/h4&gt;
&lt;p&gt;Your site’s logo. It will show on homepage and navigation menu. Also used for twitter meta tags.&lt;/p&gt;

&lt;h4 id=&quot;background&quot;&gt;background&lt;/h4&gt;
&lt;p&gt;Image for background. If you don’t set it, color will be used as a background.&lt;/p&gt;

&lt;h4 id=&quot;google-analytics-and-webmaster-tools&quot;&gt;Google Analytics and Webmaster Tools&lt;/h4&gt;

&lt;p&gt;Google Analytics UA and Webmaster Tool verification tags can be entered in &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;. For more information on obtaining these meta tags check &lt;a href=&quot;http://support.google.com/webmasters/bin/answer.py?hl=en&amp;amp;answer=35179&quot;&gt;Google Webmaster Tools&lt;/a&gt; and &lt;a href=&quot;https://ssl.bing.com/webmaster/configure/verify/ownership&quot;&gt;Bing Webmaster Tools&lt;/a&gt; support.&lt;/p&gt;

&lt;h4 id=&quot;mathjax&quot;&gt;MathJax&lt;/h4&gt;
&lt;p&gt;It’s enabled. But if you don’t want to use it. Set it false in  &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;disqus-comments&quot;&gt;Disqus Comments&lt;/h4&gt;
&lt;p&gt;Set your disqus shortname in &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; to use comments.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;navigation-links&quot;&gt;Navigation Links&lt;/h3&gt;

&lt;p&gt;To set what links appear in the top navigation edit &lt;code class=&quot;highlighter-rouge&quot;&gt;_data/navigation.yml&lt;/code&gt;. Use the following format to set the URL and title for as many links as you’d like. &lt;em&gt;External links will open in a new window.&lt;/em&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Home&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Blog&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/blog/&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Projects&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/projects/&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;About&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/about/&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Moon&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://taylantatli.me/Moon&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;layouts-and-content&quot;&gt;Layouts and Content&lt;/h2&gt;

&lt;p&gt;Moon Theme use &lt;a href=&quot;https://github.com/penibelst/jekyll-compress-html&quot;&gt;Jekyll Compress&lt;/a&gt; to compress html output. But it can cause errors if you use “linenos” (line numbers). I suggest don’t use line numbers for codes, because it won’t look good with this theme, also i didn’t give a proper style for them. If you insist to use line numbers, just remove &lt;code class=&quot;highlighter-rouge&quot;&gt;layout: compress&lt;/code&gt; string from layouts. It will disable compressing.&lt;/p&gt;

&lt;h3 id=&quot;feature-image&quot;&gt;Feature Image&lt;/h3&gt;

&lt;p&gt;You can set feature image per post. Just add &lt;code class=&quot;highlighter-rouge&quot;&gt;feature: some link&lt;/code&gt; to your post’s front matter.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;feature: /assets/img/some-image.png
or
feaure: http://example.com/some-image.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This also will be used for twitter card:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/754514/14509719/61c5751c-01d6-11e6-8c29-ce8ccad149bf.png&quot; alt=&quot;Moon Twitter Card&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;comments&quot;&gt;Comments&lt;/h3&gt;
&lt;p&gt;To show disqus comments for your post add &lt;code class=&quot;highlighter-rouge&quot;&gt;comments: true&lt;/code&gt; to your post’s front matter.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions?&lt;/h2&gt;

&lt;p&gt;Found a bug or aren’t quite sure how something works? By all means &lt;a href=&quot;https://github.com/TaylanTatli/Moon/issues/new&quot;&gt;file a GitHub Issue&lt;/a&gt;. And if you make something cool with this theme feel free to let me know.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;license&quot;&gt;License&lt;/h2&gt;

&lt;p&gt;This theme is free and open source software, distributed under the MIT License. So feel free to use this Jekyll theme on your site without linking back to me or including a disclaimer.&lt;/p&gt;</content><author><name></name></author><category term="jekyll" /><category term="moon" /><category term="blog" /><category term="about" /><category term="theme" /><summary type="html">Minimal, one column Jekyll theme for your blog.</summary></entry><entry><title type="html">Markdown Syntax</title><link href="http://localhost:4000/markdown-syntax/" rel="alternate" type="text/html" title="Markdown Syntax" /><published>2016-03-15T00:00:00-04:00</published><updated>2016-03-15T00:00:00-04:00</updated><id>http://localhost:4000/markdown-syntax</id><content type="html" xml:base="http://localhost:4000/markdown-syntax/">&lt;h2 id=&quot;html-elements&quot;&gt;HTML Elements&lt;/h2&gt;

&lt;p&gt;Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs.&lt;/p&gt;

&lt;h1 id=&quot;heading-1&quot;&gt;Heading 1&lt;/h1&gt;

&lt;h2 id=&quot;heading-2&quot;&gt;Heading 2&lt;/h2&gt;

&lt;h3 id=&quot;heading-3&quot;&gt;Heading 3&lt;/h3&gt;

&lt;h4 id=&quot;heading-4&quot;&gt;Heading 4&lt;/h4&gt;

&lt;h5 id=&quot;heading-5&quot;&gt;Heading 5&lt;/h5&gt;

&lt;h6 id=&quot;heading-6&quot;&gt;Heading 6&lt;/h6&gt;

&lt;h3 id=&quot;body-text&quot;&gt;Body text&lt;/h3&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, test link adipiscing elit. &lt;strong&gt;This is strong&lt;/strong&gt;. Nullam dignissim convallis est. Quisque aliquam.&lt;/p&gt;

&lt;p class=&quot;image-right&quot;&gt;&lt;img src=&quot;https://mmistakes.github.io/minimal-mistakes/images/3953273590_704e3899d5_m.jpg&quot; alt=&quot;Smithsonian Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is emphasized&lt;/em&gt;. Donec faucibus. Nunc iaculis suscipit dui. 53 = 125. Water is H2O. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. The New York Times (That’s a citation). Underline.Maecenas ornare tortor. Donec sed tellus eget sapien fringilla nonummy. Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus.&lt;/p&gt;

&lt;p&gt;HTML and CSS are our tools. Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus. Praesent mattis, massa quis luctus fermentum, turpis mi volutpat justo, eu volutpat enim diam eget metus.&lt;/p&gt;

&lt;h3 id=&quot;blockquotes&quot;&gt;Blockquotes&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lorem ipsum dolor sit amet, test link adipiscing elit. Nullam dignissim convallis est. Quisque aliquam.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;list-types&quot;&gt;List Types&lt;/h2&gt;

&lt;h3 id=&quot;ordered-lists&quot;&gt;Ordered Lists&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Item one
    &lt;ol&gt;
      &lt;li&gt;sub item one&lt;/li&gt;
      &lt;li&gt;sub item two&lt;/li&gt;
      &lt;li&gt;sub item three&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Item two&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;unordered-lists&quot;&gt;Unordered Lists&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Item one&lt;/li&gt;
  &lt;li&gt;Item two&lt;/li&gt;
  &lt;li&gt;Item three&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tables&quot;&gt;Tables&lt;/h2&gt;

&lt;table rules=&quot;groups&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Header1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Header2&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Header3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;cell1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cell2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cell3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;cell4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cell5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cell6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;cell1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cell2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cell3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;cell4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cell5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cell6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tfoot&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foot1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Foot2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;Foot3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tfoot&gt;
&lt;/table&gt;

&lt;h2 id=&quot;code-snippets&quot;&gt;Code Snippets&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-css&quot; data-lang=&quot;css&quot;&gt;&lt;span class=&quot;nf&quot;&gt;#container&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;margin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-240px&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;buttons&quot;&gt;Buttons&lt;/h2&gt;

&lt;p&gt;Make any link standout more when applying the &lt;code class=&quot;highlighter-rouge&quot;&gt;.btn&lt;/code&gt; class.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;#&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;btn btn-success&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Success Button&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn&quot;&gt;Primary Button&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn btn-success&quot;&gt;Success Button&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn btn-warning&quot;&gt;Warning Button&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn btn-danger&quot;&gt;Danger Button&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn btn-info&quot;&gt;Info Button&lt;/a&gt;&lt;/div&gt;

&lt;h2 id=&quot;kbd&quot;&gt;KBD&lt;/h2&gt;

&lt;p&gt;You can also use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;kbd&amp;gt;&lt;/code&gt; tag for keyboard buttons.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;kbd&amp;gt;&lt;/span&gt;W&lt;span class=&quot;nt&quot;&gt;&amp;lt;/kbd&amp;gt;&amp;lt;kbd&amp;gt;&lt;/span&gt;A&lt;span class=&quot;nt&quot;&gt;&amp;lt;/kbd&amp;gt;&amp;lt;kbd&amp;gt;&lt;/span&gt;S&lt;span class=&quot;nt&quot;&gt;&amp;lt;/kbd&amp;gt;&amp;lt;kbd&amp;gt;&lt;/span&gt;D&lt;span class=&quot;nt&quot;&gt;&amp;lt;/kbd&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Press &lt;kbd&gt;W&lt;/kbd&gt;&lt;kbd&gt;A&lt;/kbd&gt;&lt;kbd&gt;S&lt;/kbd&gt;&lt;kbd&gt;D&lt;/kbd&gt; to move your car. &lt;strong&gt;Midtown Maddness!!&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;notices&quot;&gt;Notices&lt;/h2&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Watch out!&lt;/strong&gt; You can also add notices by appending &lt;code class=&quot;highlighter-rouge&quot;&gt;{: .notice}&lt;/code&gt; to a paragraph.&lt;/p&gt;</content><author><name></name></author><category term="markdown" /><category term="syntax" /><category term="sample" /><category term="test" /><category term="jekyll" /><summary type="html">Just about everything you'll need to style in the theme: headings, paragraphs, blockquotes, tables, code blocks, and more.</summary></entry></feed>